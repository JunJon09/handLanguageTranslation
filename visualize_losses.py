import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.font_manager import FontProperties

# ãƒ‡ãƒ¼ã‚¿ã®å†èª­ã¿è¾¼ã¿
data = []
with open(r"D:\Phonenix-2014-translate\handLanguageTranslation\ContinuousSignLanguage\CNN_BiLSTM\logs\training_20250908_164346.log", 'r', encoding='utf-8', errors='ignore') as f:
    for line in f:
        if "æå¤±: Conv=" in line:
            try:
                parts = line.split("æå¤±: Conv=")[1]
                conv = float(parts.split(", Seq=")[0])
                seq = float(parts.split(", Seq=")[1].split(", Dist=")[0])
                dist = float(parts.split(", Dist=")[1].split(", ç·è¨ˆ=")[0])
                total = float(parts.split(", ç·è¨ˆ=")[1].split()[0])
                data.append([conv, seq, dist, total])
            except:
                continue

df = pd.DataFrame(data, columns=['Conv', 'Seq', 'Dist', 'Total'])

# Train/Valåˆ†é›¢
train_mask = df['Total'] > 5
val_mask = df['Total'] <= 5
train_df = df[train_mask]
val_df = df[val_mask]

# å›³ã®ã‚µã‚¤ã‚ºã‚’å¤§ããã—ã¦æ–‡å­—ã®é‡ãªã‚Šã‚’é˜²ã
plt.figure(figsize=(20, 15))  # ã•ã‚‰ã«å¤§ãã
plt.rcParams.update({'font.size': 12})  # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’èª¿æ•´

# ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ1: åˆ†å¸ƒæ¯”è¼ƒï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰
plt.subplot(2, 3, 1)
plt.hist(train_df['Total'], bins=50, alpha=0.7, label=f'Train Loss (n={len(train_df)})', color='blue', density=True)
plt.hist(val_df['Total'], bins=30, alpha=0.7, label=f'Val Loss (n={len(val_df)})', color='orange', density=True)
plt.xlabel('Total Loss', fontsize=14)
plt.ylabel('Density', fontsize=14)
plt.title('Loss Distribution Comparison\n(Normalized)', fontsize=16, pad=20)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)

# ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ2: æˆåˆ†åˆ¥æ¯”è¼ƒï¼ˆãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆï¼‰
plt.subplot(2, 3, 2)
components = ['Conv', 'Seq', 'Dist']
train_means = [train_df[comp].mean() for comp in components]
val_means = [val_df[comp].mean() for comp in components]

x = np.arange(len(components))
width = 0.35

bars1 = plt.bar(x - width/2, train_means, width, label='Train', color='blue', alpha=0.7)
bars2 = plt.bar(x + width/2, val_means, width, label='Val', color='orange', alpha=0.7)

# å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
for i, (train_val, val_val) in enumerate(zip(train_means, val_means)):
    plt.text(i - width/2, train_val + 0.5, f'{train_val:.1f}', ha='center', va='bottom', fontsize=11, fontweight='bold')
    plt.text(i + width/2, val_val + 0.1, f'{val_val:.1f}', ha='center', va='bottom', fontsize=11, fontweight='bold')

plt.xlabel('Loss Components', fontsize=14)
plt.ylabel('Average Loss Value', fontsize=14)
plt.title('Loss Components: Train vs Val\n(Average Values)', fontsize=16, pad=20)
plt.xticks(x, components, fontsize=12)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3, axis='y')

# ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ3: æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
plt.subplot(2, 3, 3)
# ãƒ‡ãƒ¼ã‚¿ãŒå¤šã™ãã‚‹ã®ã§é–“å¼•ã„ã¦ãƒ—ãƒ­ãƒƒãƒˆ
sample_rate = max(1, len(df) // 1000)  # æœ€å¤§1000ãƒã‚¤ãƒ³ãƒˆ
sampled_indices = range(0, len(df), sample_rate)

plt.plot([i for i in sampled_indices if train_mask.iloc[i]], 
         [df.iloc[i]['Total'] for i in sampled_indices if train_mask.iloc[i]], 
         'b.', alpha=0.6, markersize=3, label=f'Train (every {sample_rate}th point)')
plt.plot([i for i in sampled_indices if val_mask.iloc[i]], 
         [df.iloc[i]['Total'] for i in sampled_indices if val_mask.iloc[i]], 
         'ro', alpha=0.8, markersize=4, label=f'Val (every {sample_rate}th point)')

plt.xlabel('Iteration', fontsize=14)
plt.ylabel('Total Loss', fontsize=14)
plt.title('Loss Trends Over Time\n(Sampled Data)', fontsize=16, pad=20)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)

# ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ4: æ¯”ç‡åˆ†æ
plt.subplot(2, 3, 4)
ratios = []
labels = []
colors = []

for comp in components:
    if val_df[comp].mean() > 0:  # ã‚¼ãƒ­é™¤ç®—å›é¿
        ratio = train_df[comp].mean() / val_df[comp].mean()
        ratios.append(ratio)
        labels.append(f'{comp}\n({ratio:.1f}x)')
        
        # æ¯”ç‡ã«å¿œã˜ã¦è‰²ã‚’å¤‰æ›´
        if ratio > 5:
            colors.append('red')
        elif ratio > 2:
            colors.append('orange')
        else:
            colors.append('green')

bars = plt.bar(range(len(ratios)), ratios, color=colors, alpha=0.7)
plt.axhline(y=1, color='black', linestyle='--', alpha=0.5, label='Equal (1.0x)')
plt.axhline(y=2, color='orange', linestyle='--', alpha=0.5, label='Warning (2.0x)')
plt.axhline(y=5, color='red', linestyle='--', alpha=0.5, label='Critical (5.0x)')

# å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
for i, (bar, ratio) in enumerate(zip(bars, ratios)):
    plt.text(i, ratio + 0.2, f'{ratio:.1f}x', ha='center', va='bottom', fontsize=12, fontweight='bold')

plt.xlabel('Loss Components', fontsize=14)
plt.ylabel('Train/Val Ratio', fontsize=14)
plt.title('Overfitting Analysis\n(Train/Val Loss Ratios)', fontsize=16, pad=20)
plt.xticks(range(len(labels)), [comp for comp in components], fontsize=12)
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3, axis='y')

# ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ5: ç›¸é–¢åˆ†æ
plt.subplot(2, 3, 5)
plt.scatter(train_df['Conv'], train_df['Seq'], alpha=0.5, s=10, label='Train: Conv vs Seq', color='blue')
plt.scatter(val_df['Conv'], val_df['Seq'], alpha=0.7, s=15, label='Val: Conv vs Seq', color='orange')
plt.xlabel('Conv Loss', fontsize=14)
plt.ylabel('Seq Loss', fontsize=14)
plt.title('Conv vs Seq Loss Correlation\n(Scatter Plot)', fontsize=16, pad=20)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)

# ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ6: çµ±è¨ˆã‚µãƒãƒªãƒ¼
plt.subplot(2, 3, 6)
plt.axis('off')

# çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«
stats_text = f"""
COMPREHENSIVE LOSS ANALYSIS SUMMARY

ğŸ“Š Dataset Statistics:
â€¢ Total Entries: {len(df):,}
â€¢ Train Entries: {len(train_df):,} ({len(train_df)/len(df)*100:.1f}%)
â€¢ Val Entries: {len(val_df):,} ({len(val_df)/len(df)*100:.1f}%)

ğŸ” Average Losses:
           Train    Val     Ratio
Conv:     {train_df['Conv'].mean():.2f}    {val_df['Conv'].mean():.2f}    {train_df['Conv'].mean()/val_df['Conv'].mean():.1f}x
Seq:      {train_df['Seq'].mean():.2f}   {val_df['Seq'].mean():.2f}    {train_df['Seq'].mean()/val_df['Seq'].mean():.1f}x
Dist:     {train_df['Dist'].mean():.3f}   {val_df['Dist'].mean():.3f}   {train_df['Dist'].mean()/val_df['Dist'].mean():.1f}x
Total:    {train_df['Total'].mean():.2f}   {val_df['Total'].mean():.2f}    {train_df['Total'].mean()/val_df['Total'].mean():.1f}x

âš ï¸  Critical Issues:
â€¢ Seq Loss ratio: {train_df['Seq'].mean()/val_df['Seq'].mean():.1f}x (SEVERE)
â€¢ Overall ratio: {train_df['Total'].mean()/val_df['Total'].mean():.1f}x (HIGH)
â€¢ Dist contribution: {train_df['Dist'].mean()/train_df['Total'].mean()*100:.1f}% (LOW)

ğŸ¯ Recommendations:
1. Increase BiLSTM Dropout (0.3â†’0.5)
2. Add CNN Dropout layers
3. Increase Distillation weight (0.15â†’0.3)
4. Investigate data preprocessing differences
"""

plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=11,
         verticalalignment='top', fontfamily='monospace',
         bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))

plt.tight_layout(pad=3.0)  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’å¢—ã‚„ã—ã¦æ–‡å­—ã®é‡ãªã‚Šã‚’é˜²ã
plt.savefig('comprehensive_loss_analysis_improved.png', dpi=300, bbox_inches='tight')
plt.show()

print("ğŸ“Š æ”¹è‰¯ç‰ˆåˆ†æå›³ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ!")
print("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å: comprehensive_loss_analysis_improved.png")