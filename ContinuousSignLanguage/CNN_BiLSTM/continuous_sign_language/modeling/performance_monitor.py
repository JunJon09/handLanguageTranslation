import psutil
import time
import threading
from datetime import datetime
import torch

class PerformanceMonitor:
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, monitor_interval=1.0):
        self.monitor_interval = monitor_interval
        self.is_monitoring = False
        self.monitor_thread = None
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ç”¨
        self.cpu_usage = []
        self.memory_usage = []
        self.gpu_memory_usage = []
        self.timestamps = []
        
        self.process = psutil.Process()
        
    def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        if self.is_monitoring:
            return
            
        self.is_monitoring = True
        self.cpu_usage.clear()
        self.memory_usage.clear()
        self.gpu_memory_usage.clear()
        self.timestamps.clear()
        
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        

        
    def stop_monitoring(self):
        """ç›£è¦–åœæ­¢"""
        self.is_monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
            
        print("â¹ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’åœæ­¢ã—ã¾ã—ãŸ")
        
    def _monitor_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while self.is_monitoring:
            try:
                # ç¾åœ¨æ™‚åˆ»è¨˜éŒ²
                current_time = datetime.now()
                self.timestamps.append(current_time)
                
                # CPUä½¿ç”¨ç‡ (ãƒ—ãƒ­ã‚»ã‚¹å˜ä½)
                cpu_percent = self.process.cpu_percent()
                self.cpu_usage.append(cpu_percent)
                
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (ãƒ—ãƒ­ã‚»ã‚¹å˜ä½)
                memory_info = self.process.memory_info()
                memory_mb = memory_info.rss / 1024 / 1024  # MB
                self.memory_usage.append(memory_mb)
                
                # GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (PyTorchä½¿ç”¨æ™‚)
                gpu_memory_mb = 0
                if torch.cuda.is_available():
                    gpu_memory_bytes = torch.cuda.memory_allocated()
                    gpu_memory_mb = gpu_memory_bytes / 1024 / 1024  # MB
                
                self.gpu_memory_usage.append(gpu_memory_mb)
                
                time.sleep(self.monitor_interval)
                
            except Exception as e:
                print(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
                
    def get_summary(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã‚µãƒãƒªãƒ¼å–å¾—"""
        if not self.cpu_usage:
            return "ç›£è¦–ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"
            
        summary = {
            'monitoring_duration': len(self.timestamps) * self.monitor_interval,
            'cpu_usage': {
                'avg': sum(self.cpu_usage) / len(self.cpu_usage),
                'max': max(self.cpu_usage),
                'min': min(self.cpu_usage)
            },
            'memory_usage_mb': {
                'avg': sum(self.memory_usage) / len(self.memory_usage),
                'max': max(self.memory_usage),
                'min': min(self.memory_usage)
            },
            'gpu_memory_mb': {
                'avg': sum(self.gpu_memory_usage) / len(self.gpu_memory_usage) if self.gpu_memory_usage else 0,
                'max': max(self.gpu_memory_usage) if self.gpu_memory_usage else 0,
                'min': min(self.gpu_memory_usage) if self.gpu_memory_usage else 0
            }
        }
        
        return summary
        
    def print_summary(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã‚’è¡¨ç¤º"""
        summary = self.get_summary()
        
        if isinstance(summary, str):
            print(summary)
            return
            
        print("\n" + "="*60)
        print("ğŸ” ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–çµæœ")
        print("="*60)
        print(f"ğŸ“Š ç›£è¦–æ™‚é–“: {summary['monitoring_duration']:.1f}ç§’")
        print(f"ğŸ–¥ï¸  CPUä½¿ç”¨ç‡: å¹³å‡{summary['cpu_usage']['avg']:.1f}% | æœ€å¤§{summary['cpu_usage']['max']:.1f}% | æœ€å°{summary['cpu_usage']['min']:.1f}%")
        print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: å¹³å‡{summary['memory_usage_mb']['avg']:.0f}MB | æœ€å¤§{summary['memory_usage_mb']['max']:.0f}MB | æœ€å°{summary['memory_usage_mb']['min']:.0f}MB")
        
        if torch.cuda.is_available():
            print(f"ğŸ® GPU ãƒ¡ãƒ¢ãƒª: å¹³å‡{summary['gpu_memory_mb']['avg']:.0f}MB | æœ€å¤§{summary['gpu_memory_mb']['max']:.0f}MB | æœ€å°{summary['gpu_memory_mb']['min']:.0f}MB")
        else:
            print("ğŸ® GPU: åˆ©ç”¨ä¸å¯")
            
        print("="*60)