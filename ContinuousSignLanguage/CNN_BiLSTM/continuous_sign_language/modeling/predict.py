import CNN_BiLSTM.continuous_sign_language.modeling.functions as functions
import CNN_BiLSTM.models.cnn_bilstm_model as model
import CNN_BiLSTM.continuous_sign_language.modeling.config as model_config
import CNN_BiLSTM.continuous_sign_language.config as config
import CNN_BiLSTM.continuous_sign_language.dataset as dataset
import CNN_BiLSTM.continuous_sign_language.init_log as init_log
import CNN_BiLSTM.continuous_sign_language.modeling.performance_monitor as pm
import torch
import logging


if __name__ == "__main__":
    mode = "test"
    init_log.setup_logging(mode=mode)
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–åˆæœŸåŒ–
    monitor = pm.PerformanceMonitor(monitor_interval=1.0)
    
    logging.info("ğŸš€ ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã—ãŸ")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–é–‹å§‹
    monitor.start_monitoring()
    
    test_hdf5files, val_hdf5files, key2token = dataset.read_dataset(mode=mode)
    test_dataloader, val_dataloader, in_channels = functions.set_dataloader(key2token, test_hdf5files, val_hdf5files, mode)
    print(f"ğŸ”¢ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°: {len(test_dataloader.dataset)} ã‚µãƒ³ãƒ—ãƒ«")
    VOCAB = len(key2token)
    out_channels = VOCAB
    save_path = model_config.model_use_path
    device = "cuda" if torch.cuda.is_available() else "cpu"

    cnn_transformer = model.Model(
        vocabulary=key2token,
        in_channels=in_channels,
        hand_size=config.spatial_spatial_feature,
        cnn_out_channels=model_config.cnn_out_channels,
        cnn_dropout_rate=model_config.cnn_dropout_rate,
        conv_type=model_config.conv_type,
        use_bn=model_config.use_bn,
        kernel_sizes=model_config.kernel_sizes,
        num_layers=model_config.num_layers,
        num_heads=model_config.num_heads,
        dropout=model_config.dropout,
        num_classes=out_channels,
        blank_id=0, # CTCã®blankã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’0ã«è¨­å®š
        cnn_model_type=model_config.cnn_model_type,
        temporal_model_type=model_config.temporal_model_type,
    )

    load_model, optimizer_loaded, epoch_loaded = functions.load_model(
        cnn_transformer, save_path, device
    )

    wer, test_times = functions.test_loop(
        dataloader=test_dataloader,
        model=load_model,
        device=device,
        return_pred_times=True
    )

    print(f"ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆç²¾åº¦: {wer}")
    
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–åœæ­¢ã¨çµæœè¡¨ç¤º
    monitor.stop_monitoring()
    monitor.print_summary()
    
    # è©³ç´°ãªãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
    summary = monitor.get_summary()
    if isinstance(summary, dict):
        logging.info("=" * 50)
        logging.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–çµæœè©³ç´°")
        logging.info("=" * 50)
        logging.info(f"ç›£è¦–æ™‚é–“: {summary['monitoring_duration']:.1f}ç§’")
        logging.info(f"CPUä½¿ç”¨ç‡ - å¹³å‡: {summary['cpu_usage']['avg']:.1f}%, æœ€å¤§: {summary['cpu_usage']['max']:.1f}%, æœ€å°: {summary['cpu_usage']['min']:.1f}%")
        logging.info(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ - å¹³å‡: {summary['memory_usage_mb']['avg']:.0f}MB, æœ€å¤§: {summary['memory_usage_mb']['max']:.0f}MB, æœ€å°: {summary['memory_usage_mb']['min']:.0f}MB")
        if torch.cuda.is_available():
            logging.info(f"GPU ãƒ¡ãƒ¢ãƒª - å¹³å‡: {summary['gpu_memory_mb']['avg']:.0f}MB, æœ€å¤§: {summary['gpu_memory_mb']['max']:.0f}MB, æœ€å°: {summary['gpu_memory_mb']['min']:.0f}MB")
        else:
            logging.info("GPU: åˆ©ç”¨ä¸å¯")
        logging.info("=" * 50)
    
    logging.info("âœ… ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ")
    
   
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãŒç¢ºå®Ÿã«åœæ­¢ã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
    if 'monitor' in locals() and monitor.is_monitoring:
            monitor.stop_monitoring()
