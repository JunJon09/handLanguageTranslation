import CNN_BiLSTM.continuous_sign_language.modeling.functions as functions
import CNN_BiLSTM.models.cnn_bilstm_model as model
import CNN_BiLSTM.continuous_sign_language.modeling.config as model_config
import CNN_BiLSTM.continuous_sign_language.dataset as dataset
import CNN_BiLSTM.continuous_sign_language.init_log as init_log
import CNN_BiLSTM.continuous_sign_language.modeling.performance_monitor as pm
import torch
import logging


if __name__ == "__main__":
    mode = "test"
    init_log.setup_logging(mode=mode)
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–åˆæœŸåŒ–
    monitor = pm.PerformanceMonitor(monitor_interval=1.0)
    
    logging.info("ğŸš€ ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã—ãŸ")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–é–‹å§‹
    monitor.start_monitoring()
    
    test_hdf5files, val_hdf5files, key2token = dataset.read_dataset(mode=mode)
    test_dataloader, val_dataloader, in_channels = functions.set_dataloader(key2token, test_hdf5files, val_hdf5files, mode)
    print(f"ğŸ”¢ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°: {len(test_dataloader.dataset)} ã‚µãƒ³ãƒ—ãƒ«")
    VOCAB = len(key2token)
    out_channels = VOCAB
    save_path = model_config.model_use_path
    device = "cuda" if torch.cuda.is_available() else "cpu"

    cnn_transformer = model.CNNBiLSTMModel(
        vocabulary=key2token,
        in_channels=in_channels,
        kernel_size=model_config.kernel_size,
        cnn_out_channels=model_config.cnn_out_channels,
        stride=model_config.stride,
        padding=model_config.padding,
        dropout_rate=model_config.dropout_rate,
        bias=model_config.bias,
        resNet=model_config.resNet,
        activation=model_config.activation,
        tren_num_layers=model_config.tren_num_layers,
        tren_num_heads=model_config.tren_num_heads,
        tren_dim_ffw=model_config.tren_dim_ffw,
        tren_dropout=model_config.tren_dropout,
        tren_norm_eps=model_config.tren_norm_eps,
        batch_first=model_config.batch_first,
        tren_norm_first=model_config.tren_norm_first,
        tren_add_bias=model_config.tren_add_bias,
        num_classes=out_channels,
        blank_idx=VOCAB - 1,
        temporal_model_type=model_config.temporal_model_type,  # è¿½åŠ 
    )

    load_model, optimizer_loaded, epoch_loaded = functions.load_model(
        cnn_transformer, save_path, device
    )

    # Transformerãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®šç¢ºèª
    if (
        hasattr(model_config, "fine_tune_transformer_only")
        and model_config.fine_tune_transformer_only
    ):
        if model_config.temporal_model_type in [
            "transformer",
            "multiscale_transformer",
        ]:
            logging.info(
                "ğŸ¯ äºˆæ¸¬æ™‚ï¼šTransformerãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨"
            )
        # äºˆæ¸¬æ™‚ã¯ãƒ•ãƒªãƒ¼ã‚ºè¨­å®šã¯ä¸è¦ï¼ˆå…¨å±¤ã‚’ä½¿ç”¨ã—ã¦äºˆæ¸¬ï¼‰

    # ========================================
    # ğŸ” å¯è¦–åŒ–ãƒ»åˆ†æè¨­å®š
    # ========================================
    VISUALIZE_ATTENTION = True  # True: å¯è¦–åŒ–ã™ã‚‹, False: å¯è¦–åŒ–ã—ãªã„
    GENERATE_CONFUSION_MATRIX = True  # True: æ··åŒè¡Œåˆ—ã‚’ç”Ÿæˆ, False: ç”Ÿæˆã—ãªã„
    VISUALIZE_CONFIDENCE = True  # True: äºˆæ¸¬ä¿¡é ¼åº¦å¯è¦–åŒ–, False: å¯è¦–åŒ–ã—ãªã„
    VISUALIZE_MULTILAYER_FEATURES = True  # True: å¤šå±¤ç‰¹å¾´é‡å¯è¦–åŒ–, False: å¯è¦–åŒ–ã—ãªã„
    MULTILAYER_METHOD = "both"  # "tsne", "umap", "both"

    if (
        VISUALIZE_ATTENTION
        or GENERATE_CONFUSION_MATRIX
        or VISUALIZE_CONFIDENCE
        or VISUALIZE_MULTILAYER_FEATURES
    ):
        analysis_options = []
        if VISUALIZE_ATTENTION:
            analysis_options.append("Attention & CTCå¯è¦–åŒ–")
        if GENERATE_CONFUSION_MATRIX:
            analysis_options.append("æ··åŒè¡Œåˆ—åˆ†æ")
        if VISUALIZE_CONFIDENCE:
            analysis_options.append("äºˆæ¸¬ä¿¡é ¼åº¦å¯è¦–åŒ–")
        if VISUALIZE_MULTILAYER_FEATURES:
            analysis_options.append(f"å¤šå±¤ç‰¹å¾´é‡å¯è¦–åŒ–({MULTILAYER_METHOD})")

        print(f"ğŸ” æ‹¡å¼µåˆ†æãƒ¢ãƒ¼ãƒ‰ã§ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™")
        print(f"  æœ‰åŠ¹ãªåˆ†æ: {', '.join(analysis_options)}")
        print(
            f"  å¤šå±¤ç‰¹å¾´é‡åˆ†æ: CNNç©ºé–“ãƒ‘ã‚¿ãƒ¼ãƒ³ã€BiLSTMæ™‚ç³»åˆ—ã€Attentioné‡è¦åº¦ã€æœ€çµ‚çµ±åˆç‰¹å¾´é‡"
        )

        wer, test_times = functions.test_loop(
            dataloader=test_dataloader,
            model=load_model,
            device=device,
            return_pred_times=True,
            blank_id=VOCAB - 1,
            visualize_attention=VISUALIZE_ATTENTION,
            generate_confusion_matrix=GENERATE_CONFUSION_MATRIX,
            visualize_confidence=VISUALIZE_CONFIDENCE,
            visualize_multilayer_features=VISUALIZE_MULTILAYER_FEATURES,
            multilayer_method=MULTILAYER_METHOD,
        )
    else:
        print("ğŸ“Š é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™")
        wer, test_times = functions.test_loop(
            dataloader=test_dataloader,
            model=load_model,
            device=device,
            return_pred_times=True,
            blank_id=VOCAB - 1,
            visualize_attention=False,
            generate_confusion_matrix=False,
            visualize_confidence=False,
            visualize_multilayer_features=False,
            multilayer_method="both",
        )

    print(f"ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆç²¾åº¦: {wer}")
    
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–åœæ­¢ã¨çµæœè¡¨ç¤º
    monitor.stop_monitoring()
    monitor.print_summary()
    
    # è©³ç´°ãªãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
    summary = monitor.get_summary()
    if isinstance(summary, dict):
        logging.info("=" * 50)
        logging.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–çµæœè©³ç´°")
        logging.info("=" * 50)
        logging.info(f"ç›£è¦–æ™‚é–“: {summary['monitoring_duration']:.1f}ç§’")
        logging.info(f"CPUä½¿ç”¨ç‡ - å¹³å‡: {summary['cpu_usage']['avg']:.1f}%, æœ€å¤§: {summary['cpu_usage']['max']:.1f}%, æœ€å°: {summary['cpu_usage']['min']:.1f}%")
        logging.info(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ - å¹³å‡: {summary['memory_usage_mb']['avg']:.0f}MB, æœ€å¤§: {summary['memory_usage_mb']['max']:.0f}MB, æœ€å°: {summary['memory_usage_mb']['min']:.0f}MB")
        if torch.cuda.is_available():
            logging.info(f"GPU ãƒ¡ãƒ¢ãƒª - å¹³å‡: {summary['gpu_memory_mb']['avg']:.0f}MB, æœ€å¤§: {summary['gpu_memory_mb']['max']:.0f}MB, æœ€å°: {summary['gpu_memory_mb']['min']:.0f}MB")
        else:
            logging.info("GPU: åˆ©ç”¨ä¸å¯")
        logging.info("=" * 50)
    
    logging.info("âœ… ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ")
    
   
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãŒç¢ºå®Ÿã«åœæ­¢ã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
    if 'monitor' in locals() and monitor.is_monitoring:
            monitor.stop_monitoring()
