import CNN_BiLSTM.continuous_sign_language.modeling.functions as functions
import CNN_BiLSTM.models.cnn_bilstm_model as model
import CNN_BiLSTM.continuous_sign_language.modeling.config as model_config
import CNN_BiLSTM.continuous_sign_language.config as config
import CNN_BiLSTM.continuous_sign_language.dataset as dataset
import torch
import torch.nn as nn
import coremltools as ct
import numpy as np

# --- 1. ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ ---
mode = "test"
test_hdf5files, val_hdf5files, key2token = dataset.read_dataset(mode=mode)
# VOCABã‚µã‚¤ã‚ºï¼ˆBlankå«ã‚€ï¼‰ã®ç¢ºå®š
VOCAB = len(key2token)

# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ– (model.Modelã®å¼•æ•°ã«ã™ã¹ã¦ã®è¨­å®šã‚’æ¸¡ã™)
# ã“ã“ã§ model_instance ãŒ PyTorch ã® nn.Module ã«ãªã‚Šã¾ã™
model_instance = model.Model(
    vocabulary=key2token,
    in_channels=196,  # ä½ç½®æƒ…å ±ã®å…¥åŠ›æ¬¡å…ƒ
    hand_size=config.spatial_spatial_feature,  # è·é›¢æƒ…å ±ã®å…¥åŠ›æ¬¡å…ƒ
    cnn_out_channels=model_config.cnn_out_channels,
    cnn_dropout_rate=model_config.cnn_dropout_rate,
    conv_type=model_config.conv_type,
    use_bn=model_config.use_bn,
    kernel_sizes=model_config.kernel_sizes,
    num_layers=model_config.num_layers,
    num_heads=model_config.num_heads,
    dropout=model_config.dropout,
    num_classes=VOCAB,
    blank_id=0,
    cnn_model_type=model_config.cnn_model_type,
    temporal_model_type=model_config.temporal_model_type,
)

# 2. å­¦ç¿’æ¸ˆã¿é‡ã¿ã®ãƒ­ãƒ¼ãƒ‰
MODEL_PATH = model_config.model_use_path
device = torch.device("cpu")

# é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹éš›ã¯ã€å¿…ãš eval ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®šã—ã¾ã™
try:
    checkpoint = torch.load(MODEL_PATH, map_location=device)
    if "model_state_dict" in checkpoint:
        model_instance.load_state_dict(checkpoint["model_state_dict"])
        print("âœ… model_state_dictã‹ã‚‰é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
    else:
        model_instance.load_state_dict(checkpoint)
        print("âœ… é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {MODEL_PATH}")
except Exception as e:
    print(f"âš ï¸ é‡ã¿ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ï¼‰: {e}")

model_instance.eval()


# --- 2.5. Core MLç”¨ã®ãƒ©ãƒƒãƒ‘ãƒ¼ãƒ¢ãƒ‡ãƒ«ä½œæˆ ---
class CoreMLWrapper(nn.Module):
    """Core MLå¤‰æ›ç”¨ã®ãƒ©ãƒƒãƒ‘ãƒ¼ãƒ¢ãƒ‡ãƒ«ï¼ˆæ¨è«–å°‚ç”¨ã€logitsã®ã¿è¿”ã™ï¼‰"""

    def __init__(self, model):
        super().__init__()
        self.model = model
        # PackedSequenceã‚’ä½¿ã‚ãªã„ã‚ˆã†ã«ã™ã‚‹
        # BiLSTMã®RNNå±¤ã‚’ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã«ã™ã‚‹
        if hasattr(model.temporal_model, "rnn"):
            self.rnn = model.temporal_model.rnn
        self.cnn_model = model.cnn_model
        self.spatial_correlation = model.spatial_correlation
        self.classifier = model.classifier
        self.cnn_model_type = model.cnn_model_type

    def forward(self, src_feature, spatial_feature, input_lengths):
        # å…ƒã®ãƒ¢ãƒ‡ãƒ«ã®forwardå‡¦ç†ã‚’æ‰‹å‹•ã§å®Ÿè¡Œï¼ˆPackedSequenceã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        N, C, T, J = src_feature.shape
        src_feature_reshaped = (
            src_feature.permute(0, 3, 1, 2).contiguous().view(N, C * J, T)
        )
        spatial_feature_reshaped = spatial_feature.permute(0, 2, 1)

        # CNNãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œ
        if self.cnn_model_type == "DualCNNWithCTC":
            cnn_out, cnn_logit, updated_lgt = self.cnn_model(
                skeleton_feat=src_feature_reshaped,
                hand_feat=spatial_feature_reshaped,
                lgt=input_lengths,
            )
        else:
            cnn_out, cnn_logit, updated_lgt = self.cnn_model(
                skeleton_feat=src_feature_reshaped,
                spatial_feature=spatial_feature_reshaped,
                lgt=input_lengths,
            )

        # ç›¸é–¢å­¦ç¿’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆæ³¨æ„æ©Ÿæ§‹ã®å¯è¦–åŒ–ãªã—ï¼‰
        cnn_out = self.spatial_correlation(cnn_out)

        # BiLSTM/Transformerã®å®Ÿè¡Œï¼ˆPackedSequenceãªã—ï¼‰
        # cnn_outã¯[T, B, C]å½¢å¼
        # RNNã‚’ç›´æ¥å‘¼ã³å‡ºã™ï¼ˆpack_padded_sequenceã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        rnn_outputs, _ = self.rnn(cnn_out)  # PackedSequenceä¸ä½¿ç”¨

        # åˆ†é¡å™¨ã®å®Ÿè¡Œ
        outputs = self.classifier(rnn_outputs)

        # logitsãƒ†ãƒ³ã‚½ãƒ«ã®ã¿è¿”ã™ï¼ˆãƒ‡ã‚³ãƒ¼ãƒ‰çµæœã¯å«ã¾ãªã„ï¼‰
        return outputs


wrapped_model = CoreMLWrapper(model_instance)
wrapped_model.eval()

# --- 3. Core ML å¤‰æ›ã®è¨­å®š ---

# ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼ˆæ™‚é–“è»¸ï¼‰ã‚’å¯å¤‰ï¼ˆ1ã€œ500ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰ã«è¨­å®š
flexible_time = ct.RangeDim(lower_bound=1, upper_bound=500, default=150)

# ãƒ¢ãƒ‡ãƒ«ã¯ [B, C, T, J] å½¢å¼ã‚’æœŸå¾… (196 = C * J = 2 * 98)
# Swiftå´ã§ã¯ [B, T, C*J] å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨æ„ã—ã€
# ãƒ¢ãƒ‡ãƒ«å†…ã§è‡ªå‹•çš„ã« reshape ã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
inputs = [
    ct.TensorType(name="src_feature", shape=(1, 2, flexible_time, 98)),  # [B, C, T, J]
    ct.TensorType(
        name="spatial_feature", shape=(1, flexible_time, 24)
    ),  # [B, T, spatial_dim]
    ct.TensorType(name="input_lengths", shape=(1,), dtype=np.int32),  # [B]
]

# 4. TorchScriptã¸ã®ãƒˆãƒ¬ãƒ¼ã‚¹
print("ğŸ”§ TorchScriptã«ãƒˆãƒ¬ãƒ¼ã‚¹ä¸­...")
# ãƒ€ãƒŸãƒ¼å…¥åŠ›ã®ä½œæˆ
# ãƒ¢ãƒ‡ãƒ«ã¯ [batch, C, T, J] å½¢å¼ã‚’æœŸå¾… (196 = C * J = 2 * 98)
dummy_src_feature = torch.randn(1, 2, 150, 98)  # [B, C, T, J]
dummy_spatial_feature = torch.randn(1, 150, 24)  # [B, T, spatial_dim]
dummy_input_lengths = torch.tensor([150], dtype=torch.long)  # [B]

# ãƒ©ãƒƒãƒ‘ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ï¼ˆ3ã¤ã®å¼•æ•°ã®ã¿ï¼‰
# strict=Falseã§æ¡ä»¶åˆ†å²ã®è­¦å‘Šã‚’æŠ‘åˆ¶
traced_model = torch.jit.trace(
    wrapped_model,
    (dummy_src_feature, dummy_spatial_feature, dummy_input_lengths),
    strict=False,
)

# 5. Core ML ã¸ã®å¤‰æ›å®Ÿè¡Œ
print("ğŸš€ PyTorchãƒ¢ãƒ‡ãƒ«ã‚’Core MLã«å¤‰æ›ä¸­...")
mlmodel = ct.convert(
    traced_model,
    inputs=inputs,
    minimum_deployment_target=ct.target.iOS16,
    convert_to="mlprogram",
)

# 6. ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä»˜ä¸
mlmodel.author = "Jun Shibata"
mlmodel.license = "Master's Thesis Project"
mlmodel.short_description = (
    "2-Stream CNN-BiLSTM for Sign Language Recognition (220-dim features)"
)

# 7. ä¿å­˜
mlmodel.save("SignLanguageModel.mlpackage")
print("âœ¨ ä¿å­˜å®Œäº†: SignLanguageModel.mlpackage")
